{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Function fitting and Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's session, we will be using some of the LAI datasets we examined last week (masked by national boundaries) and doing some analysis on them.\n",
    "\n",
    "- [5.1 Making 3D datasets and Movies](#5.1-Making-3D-datasets-and-Movies)\n",
    "    First, we will examine how to improve our data reading function by extracting only the area we are interested in. This involves querying the 'country' mask to find its limits and passing this information through to the reader.\n",
    "\n",
    "- [5.2 Interpolation](#5.2-Interpolation)\n",
    "    Then we will look at methods to interpolate and smooth over gaps in datasets using various methods.\n",
    "\n",
    "- [5.3 Function Fitting](#5.3-Function-fitting)\n",
    "    Finally, we will look at fitting models to datasets, in this case a model describing LAI phenology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Making 3D datasets and Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First though, we will briefly go over once more the work we did on downloading data (ussssing `wget`), generating 3D masked datasets, and making movies.\n",
    "\n",
    "This time, we will concentrate more on generating functions that we can re-use for other purposes.\n",
    "\n",
    "### 5.1.1 Downloading data\n",
    "\n",
    "We can download data from the MODIS server. The URL for this is [http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/](http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/), and you can see that inside that URL, we have a folder per time step. Inside that folder, all the files for each of the land tiles are present. Currently, you need an username and password to access this folders, which you can [register for here](https://urs.earthdata.nasa.gov/). You should then be able to click on the links and see and download individual files.\n",
    "\n",
    "Let's try to write some Python code to download individual files. The code will try to do the following:\n",
    "\n",
    "1. Open the URL [http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/2013.02.18](http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/2013.02.18), and retrieve the HTML content using the `requests` package\n",
    "2. Parse the content of the HTML to extract the URLs from all the available links using the BeautifulSoup package.\n",
    "3. Check that the files have our tile of interest, as well as end in `.hdf`\n",
    "\n",
    "Let's see how this works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# The following two modules allow you to...\n",
    "import bs4 # Parse HTML data\n",
    "import requests     # Get data on the internet using an URL\n",
    "\n",
    "the_url = 'http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/'\n",
    "tile = \"h31v11\"\n",
    "dates = \"2013.02.18\" # Eg.\n",
    "\n",
    "r = requests.get (the_url + dates)\n",
    "if not r.ok:\n",
    "    print \"Problem!\"\n",
    "html_content = r.content\n",
    "\n",
    "soup = bs4.BeautifulSoup(html_content)\n",
    "for link in soup.findAll(\"a\"):\n",
    "    fname = link.get(\"href\")\n",
    "    if fname.find (tile) >= 0 and fname.split(\".\")[-1] == \"hdf\":\n",
    "        print fname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Read multiple dates\n",
    "\n",
    "The previous code is OK for a single date, but we can make it even more useful if we stick it in a function to download data for a particular date. \n",
    "\n",
    "We might want to check that the date is a valid date for the particular dataset that we're targetting...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_modis_fname ( modis_product_url='http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/',\n",
    "                    tile=\"h31v11\", date=\"2013.02.18\"):\n",
    "    \"\"\"Get the URL for a MODIS Coll5 HDF file for a given tile and date.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    modis_product_url: str\n",
    "        A valid URL that points to the MODIS product\n",
    "    tile: str\n",
    "        A tile in MODIS parlance (e.g. \"h17v03\")\n",
    "    date: str\n",
    "        A date, in year.month.date\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    Either a full URL to download, or None if the e.g. \n",
    "    the date isn't available.\n",
    "    \"\"\"\n",
    "    import bs4\n",
    "    import requests\n",
    "    \n",
    "    r = requests.get ( modis_product_url + \"/\" + date )\n",
    "    if not r.ok:\n",
    "        # A problem was found. Warn the user and return None\n",
    "        print \"There was a problem obtaining the URL %s\" % ( the_url + date )\n",
    "        return None\n",
    "    html_content = r.content\n",
    "\n",
    "    soup = bs4.BeautifulSoup(html_content)\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        fname = link.get(\"href\")\n",
    "        if fname.find (tile) >= 0 and fname.split(\".\")[-1] == \"hdf\":\n",
    "            return modis_product_url + \"/\" + date + \"/\" + fname \n",
    "                         # We assume we only have a single file fitting the \n",
    "                         # previous criteria\n",
    "                \n",
    "                \n",
    "                \n",
    "print get_modis_fname ( modis_product_url='http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/',\n",
    "                    tile=\"h31v11\", date=\"2013.02.18\")\n",
    "print get_modis_fname ( modis_product_url='http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/',\n",
    "                    tile=\"h31v11\", date=\"2013.02.19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve the file, we need a username and password (see above), and we need to authenticate ourselves in order to download the data. The following code (taken from [get_modis](http://github.com/jgomezdans/get_modis) deals with the authentication, but also does some other useful stuff:\n",
    "\n",
    "* Requests the file size from the remote server\n",
    "* Works out how long it took to download the data\n",
    "* Uses a *context manager* to open a file.\n",
    "\n",
    "Perhaps the context manager is the weirdest concept here, but it's just a safe and convenient way of opening and closing a file. The following code\n",
    "    \n",
    "```python\n",
    "    fp = open (\"myfile.txt\", 'w')\n",
    "    fp.write ( x )\n",
    "    fp.close()\n",
    "```\n",
    "    \n",
    "can be written as\n",
    "\n",
    "```python\n",
    "    with open(\"myfile.txt\", 'w') as fp:\n",
    "        fp.write ( x )\n",
    "```\n",
    "\n",
    "The second option is preferred if you're doing more complicated stuff than just writing, as if something fails inside the context manager, `fp` will always be closed.\n",
    "\n",
    "Note that we also use a context manager to authenticate ourselves against the MODIS server. The way the authentication works is through a redirection: you ask for the URL, and the server tells you to go somewhere else for authentication and then redirects you back to your HDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_modis_product ( url, username, password, out_dir=\"./\"):\n",
    "    \"\"\"\"\n",
    "    Download a MODIS product from a given URL. This method requires an\n",
    "    username and password, as well as an output directory (optional) to\n",
    "    save the file. The method will authenticate against the server, get\n",
    "    the data, and report some timing statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    \n",
    "    url: str\n",
    "        The URL. Must exist, otherwise, we raise an IOError exception\n",
    "    username: str\n",
    "        The useranme. I bet that was surprising.\n",
    "    password: str\n",
    "        The day of the week. Naaaah, the password!!!\n",
    "    out_dir: str\n",
    "        The output directory where the file will be downloaded.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    Nothing, it reports some speed statistics on the command line.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import os\n",
    "    import time\n",
    "    # The following is an HTTP session, denoted by `s`\n",
    "    with requests.Session() as s:\n",
    "        # Set up the authentication\n",
    "        s.auth = (username, password)\n",
    "        # Get the named URL from the user. This will \n",
    "        # result in an HTTP redirection for which we\n",
    "        # need to harvest the url\n",
    "        r1 = s.request('get', url)\n",
    "        # The redirection is stored in the .url bit\n",
    "        # We also open as a stream for downloading larger\n",
    "        # files using an iterator.\n",
    "        r = s.get(r1.url, stream=True)\n",
    "        # Check whether it all worked\n",
    "        if not r.ok:\n",
    "            raise IOError(\"Can't start download... [%s]\" % url)\n",
    "        # Get the file size from the headers. File size is in bytes\n",
    "        file_size = int(r.headers['content-length'])\n",
    "        # Get the filename from the url\n",
    "        fname = url.split(\"/\")[-1]\n",
    "        print \"Starting download on %s(%d bytes) ...\" % ( \n",
    "            os.path.join(out_dir, fname), file_size)\n",
    "        # The next line stores the current time in seconds. It's our\n",
    "        # start time\n",
    "        tic = int(round(time.time() ))\n",
    "        # Open the output file to write\n",
    "        with open(os.path.join(out_dir, fname), 'wb') as fp:\n",
    "            # This iterates over the remote file contents in\n",
    "            # 64kbytes chunks. If the chunk is valid, we just\n",
    "            # write to the output file\n",
    "            for chunk in r.iter_content(chunk_size=65535):\n",
    "                if chunk:\n",
    "                    fp.write(chunk)\n",
    "        # Time at the end\n",
    "        toc = int(round(time.time() ))\n",
    "        print \"Done in %d s. Speed = %g bytes/second\" % ( \n",
    "            (toc-tic), float(file_size)/ (toc-tic))\n",
    "        \n",
    "        \n",
    "the_url = get_modis_fname ()    \n",
    "username = \"ProfLewis\"\n",
    "password = \"GeogG1222016\"\n",
    "get_modis_product ( the_url, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Read Just The Data We Want\n",
    "\n",
    "Last time, we generated a function to read MODIS LAI data.\n",
    "\n",
    "We have now included such a function in the directory [`files/python`](files/python) called [`get_lai.py`](files/python/get_lai.py).\n",
    "\n",
    "The only added sophistication is that when we call `ReadAsArray`, we give it the starting cols, rows, and number of cols and rows to read (e.g. `xsize=600,yoff=300,xoff=300,ysize=600`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "filelist = glob.glob(\"data/MCD15A2.A2005*hdf\")\n",
    "\n",
    "# Now we have a list of filenames\n",
    "# load read_lai\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "\n",
    "from get_lai import get_lai\n",
    "\n",
    "help(get_lai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. for reading a single file:\n",
    "\n",
    "lai_file0 = get_lai(filelist[20],ncol=600,mincol=300,minrow=400,nrow=800)\n",
    "plt.imshow(lai_file0['Lai_1km'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(lai_file0)\n",
    "print lai_file0.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary with has keys `['Lai_1km', 'LaiStdDev_1km', 'FparLai_QC']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lai_file0['Lai_1km'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these datasets is of shape `(1200, 1200)`, but we have read only 600 (columns) and 800 (rows) in this case. Note that the numpy indexing is `(rows,cols)`.\n",
    "\n",
    "We know how to create a mask from a vector dataset from thelast session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to make sure have access to gdal data files \n",
    "import os\n",
    "if 'GDAL_DATA' not in os.environ:\n",
    "    os.environ[\"GDAL_DATA\"] = '/opt/anaconda/share/gdal'\n",
    "\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "# make a raster mask\n",
    "# from the layer IRELAND in world.shp\n",
    "filename = filelist[0]\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template%(filename,'Lai_1km')\n",
    "                           \n",
    "mask = raster_mask(file_spec,\n",
    "                   target_vector_file = \"data/world.shp\",\\\n",
    "                   attribute_filter = \"NAME = 'IRELAND'\")\n",
    "\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the data we want is only a small section of the whole spatial dataset.\n",
    "\n",
    "It would be convenient to extract *only* the part we want.\n",
    "\n",
    "We can use `numpy.where()` to help with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The mask is False for the area we want\n",
    "rowpix,colpix = np.where(mask == False)\n",
    "\n",
    "print rowpix,colpix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rowpix` and `colpix` are lists of pixel coordinates where the condition we specified is `True` (i.e. where `mask` is `False`).\n",
    "\n",
    "If we wanted to find the bounds of this area, we simply need to know the minimum and maximum column and row in these lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mincol,maxcol = min(colpix),max(colpix)\n",
    "minrow,maxrow = min(rowpix),max(rowpix)\n",
    "\n",
    "# think about why the + 1 here!!!\n",
    "# what if maxcol and mincol were the same?\n",
    "ncol = maxcol - mincol + 1\n",
    "nrow = maxrow - minrow + 1\n",
    "\n",
    "print minrow,mincol,nrow,ncol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this information to extract *only* the area we want when we read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lai_file0 = get_lai(filelist[20],\\\n",
    "                    ncol=ncol,nrow=nrow,mincol=mincol,minrow=minrow)\n",
    "\n",
    "plt.imshow(lai_file0['Lai_1km'],interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets extract this portion of the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "plt.imshow(small_mask,interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine the country mask with the small dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap, we can use the function `raster_mask` that we gave you last time to develop a raster mask (!) from an ESRI shapefile (`data/world.shp` here).\n",
    "\n",
    "We can then combine this mask with the QC-derived mask in the LAI dataset.\n",
    "\n",
    "The LAI mask (that will be `lai.mask` in the code below) is `False` for good data, as is the coutry mask.\n",
    "\n",
    "To combine them, we want some operator `X` for which:\n",
    "\n",
    "`True  X True  == True`  \n",
    "`True  X False == True`  \n",
    "`False X True  == True`  \n",
    "`False X False == False`  \n",
    "\n",
    "The operator to use then is an *or*, here, a bitwise or, `|`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "lai_file0 = get_lai(filelist[20],\n",
    "                    ncol=ncol,nrow=nrow,mincol=mincol,minrow=minrow)\n",
    "\n",
    "layer = 'Lai_1km'\n",
    "lai = lai_file0[layer]\n",
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "# combined mask\n",
    "new_mask = small_mask | lai.mask\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(new_mask,interpolation='none')\n",
    "\n",
    "lai = ma.array(lai,mask=new_mask)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(lai,interpolation='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be used to writing loops around such functions.\n",
    "\n",
    "In this case, we read *all* of the files in `filelist` and put the data into the dictionary called `lai` here.\n",
    "\n",
    "Because there are multiple layers in the datasets, we loop over layer and append to each list indiviually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load 'em all ...\n",
    "\n",
    "# for United Kingdom here\n",
    "\n",
    "import numpy.ma as ma\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "country = 'UNITED KINGDOM'\n",
    "\n",
    "# make a raster mask\n",
    "# from the layer UNITED KINGDOM in world.shp\n",
    "filename = filelist[0]\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template%(filename,'Lai_1km')\n",
    "                           \n",
    "mask = raster_mask(file_spec,\n",
    "                   target_vector_file = \"data/world.shp\",\n",
    "                   attribute_filter = \"NAME = '%s'\"%country)\n",
    "# extract just the area we want\n",
    "# by getting the min/max rows/cols\n",
    "# of the data mask\n",
    "# The mask is False for the area we want\n",
    "rowpix,colpix = np.where(mask == False)\n",
    "mincol,maxcol = min(colpix),max(colpix)\n",
    "minrow,maxrow = min(rowpix),max(rowpix)\n",
    "ncol = maxcol - mincol + 1\n",
    "nrow = maxrow - minrow + 1\n",
    "# and make a small mask\n",
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "\n",
    "# data_fields with empty lists\n",
    "data_fields = {'LaiStdDev_1km':[],'Lai_1km':[]}\n",
    "\n",
    "# make a dictionary and put the filenames in it\n",
    "# along with the mask and min/max info\n",
    "lai = {'filenames':np.sort(filelist),\n",
    "       'minrow':minrow,'mincol':mincol,\n",
    "       'mask':small_mask}\n",
    "\n",
    "# combine the dictionaries\n",
    "lai.update(data_fields)\n",
    "\n",
    "# loop over each filename\n",
    "for f in np.sort(lai['filenames']):\n",
    "    this_lai = get_lai(f,\n",
    "                       mincol=mincol,ncol=ncol,\n",
    "                       minrow=minrow,nrow=nrow)\n",
    "    for layer in data_fields.keys():\n",
    "        # apply the mask\n",
    "        new_mask = this_lai[layer].mask | small_mask\n",
    "        this_lai[layer] = ma.array(this_lai[layer],mask=new_mask)\n",
    "        lai[layer].append(this_lai[layer])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have a look at one of these\n",
    "\n",
    "i = 20\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'][i].shape\n",
    "\n",
    "root = 'images/lai_uk'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "f = lai['filenames'][i]\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "# get some info from filename\n",
    "file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "print file_id\n",
    "plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "# plot a jpg\n",
    "plt.title(file_id)\n",
    "plt.colorbar()\n",
    "plt.savefig('images/lai_uk_%s.jpg'%file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thats quite good, so put as a function:\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "from get_lai import get_lai\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "\n",
    "def read_lai(filelist,country=None):\n",
    "    '''\n",
    "    Read MODIS LAI data from a set of files\n",
    "    in the list filelist. Data assumed to be in\n",
    "    directory datadir.\n",
    "    \n",
    "    Parameters:\n",
    "    filelist : list of LAI files\n",
    "    \n",
    "    Options:\n",
    "    country  : country name (in data/world.shp)\n",
    "    \n",
    "    Returns:\n",
    "    lai dictionary\n",
    "    '''\n",
    "    if country:\n",
    "        # make a raster mask\n",
    "        # from the layer UNITED KINGDOM in world.shp\n",
    "        file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "        file_spec = file_template%(filelist[0],'Lai_1km')\n",
    "                                   \n",
    "        mask = raster_mask(file_spec,\\\n",
    "                           target_vector_file = \"data/world.shp\",\\\n",
    "                           attribute_filter = \"NAME = '%s'\"%country)\n",
    "        # extract just the area we want\n",
    "        # by getting the min/max rows/cols\n",
    "        # of the data mask\n",
    "        # The mask is False for the area we want\n",
    "        rowpix,colpix = np.where(mask == False)\n",
    "        mincol,maxcol = min(colpix),max(colpix)\n",
    "        minrow,maxrow = min(rowpix),max(rowpix)\n",
    "        ncol = maxcol - mincol + 1\n",
    "        nrow = maxrow - minrow + 1\n",
    "        # and make a small mask\n",
    "        small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "    else:\n",
    "        # no country\n",
    "        mincol = 0\n",
    "        maxcol = 0\n",
    "        ncol = None\n",
    "        nrow = None\n",
    "\n",
    "    # data_fields with empty lists\n",
    "    data_fields = {'LaiStdDev_1km':[],'Lai_1km':[]}\n",
    "    \n",
    "    # make a dictionary and put the filenames in it\n",
    "    # along with the mask and min/max info\n",
    "    lai = {'filenames':np.sort(filelist),\n",
    "           'minrow':minrow,'mincol':mincol,\n",
    "           'mask':small_mask}\n",
    "    \n",
    "    # combine the dictionaries\n",
    "    lai.update(data_fields)\n",
    "    \n",
    "    # loop over each filename\n",
    "    for f in np.sort(lai['filenames']):\n",
    "        this_lai = get_lai(f,\n",
    "                           mincol=mincol,ncol=ncol,\n",
    "                           minrow=minrow,nrow=nrow)\n",
    "        for layer in data_fields.keys():\n",
    "            # apply the mask\n",
    "            if country:\n",
    "                new_mask = this_lai[layer].mask | small_mask\n",
    "                this_lai[layer] = ma.array(this_lai[layer],mask=new_mask)\n",
    "            lai[layer].append(this_lai[layer])   \n",
    "    for layer in data_fields.keys():\n",
    "        lai[layer] = ma.array(lai[layer])\n",
    "            \n",
    "    return lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test this ... the one in the file\n",
    "# does a cutout of the data area as well\n",
    "# which will keep the memory\n",
    "# requirements down\n",
    "#from get_lai import read_lai\n",
    "\n",
    "lai = read_lai(filelist,country='IRELAND')\n",
    "\n",
    "# have a look at one of these\n",
    "\n",
    "i = 20\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'][i].shape\n",
    "\n",
    "root = 'images/lai_eire'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "f = lai['filenames'][i]\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "# get some info from filename\n",
    "file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "print file_id\n",
    "plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "# plot a jpg\n",
    "plt.title(file_id)\n",
    "plt.colorbar()\n",
    "plt.savefig('%s_%s.jpg'%(root,file_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a movie\n",
    "\n",
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'].shape\n",
    "\n",
    "root = 'images/lai_country_eire'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "for i,f in enumerate(lai['filenames']):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    # get some info from filename\n",
    "    file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "    print file_id\n",
    "    plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "    # plot a jpg\n",
    "    plt.title(file_id)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "    plt.close(fig)\n",
    "    \n",
    "cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie.gif'.format(root)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_country_eire_movie.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The movie making works, so pack that into a function\n",
    "\n",
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "root = 'images/lai_eire'\n",
    "\n",
    "def make_movie(lai,root,layer='Lai_1km',vmax=4.,vmin=0.,do_plot=False):\n",
    "    '''\n",
    "    Make an animated gif from MODIS LAI data in\n",
    "    dictionary 'lai'.\n",
    "    \n",
    "    Parameters:\n",
    "    lai    : data dictionary\n",
    "    root   : root file /directory name of frames and movie\n",
    "    \n",
    "    layer  : data layer to plot \n",
    "    vmax   : max value for plotting\n",
    "    vmin   : min value for plotting\n",
    "    do_plot: set True if you want the individual plots\n",
    "             to display\n",
    "    \n",
    "    Returns:\n",
    "    movie name    \n",
    "    \n",
    "    '''\n",
    "    cmap = plt.cm.Greens\n",
    "    \n",
    "    for i,f in enumerate(lai['filenames']):\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        # get some info from filename\n",
    "        file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "        print file_id\n",
    "        plt.imshow(lai[layer][i],cmap=cmap,interpolation='none',\\\n",
    "                   vmax=vmax,vmin=vmin)\n",
    "        # plot a jpg\n",
    "        plt.title(file_id)\n",
    "        plt.colorbar()\n",
    "        plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "        if not do_plot:\n",
    "            plt.close(fig)\n",
    "        \n",
    "    cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie.gif'.format(root)\n",
    "    os.system(cmd)\n",
    "    return '{0}_movie.gif'.format(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "\n",
    "lai_uk = read_lai(filelist,country='UNITED KINGDOM')\n",
    "root = 'images/lai_UK'\n",
    "movie = make_movie(lai_uk,root)\n",
    "print movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_UK_movie.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Univariate interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can load the data we want from multiple MODIS hdf files that we have downloaded from the NASA server into a 3D masked numpy array, with a country boundary mask (projected int the raster data coordinate system) from a vector dataset.\n",
    "\n",
    "Let's start to explore the data then.\n",
    "\n",
    "You should have an array of LAI for Ireland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(lai['Lai_1km'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the LAI for some given pixels.\n",
    "\n",
    "First, we might like to identify which pixels actually have any data.\n",
    "\n",
    "A convenient function for this would be `np.where` that returns the indices of items that are `True`.\n",
    "\n",
    "Since the data mask is `False` for good data, we take the complement `~` so that good data are `True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = lai['Lai_1km']\n",
    "np.where(~data.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example good pixel this is (3,329,145). Let's look at this for all time periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = lai['Lai_1km']\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(np.arange(len(pixel))*8,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(np.arange(len(pixel))*8,pixel,'k--')\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data follow the trend of what we might expect for LAI development, but they are clearly a little noisy.\n",
    "\n",
    "We also have access to uncertainty information (standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy the data in case we change it any\n",
    "\n",
    "data = lai['Lai_1km'].copy()\n",
    "sd   = lai['LaiStdDev_1km'].copy()\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel    = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(len(pixel))*8\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(x,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(x,pixel,'k--')\n",
    "# plot error bars:\n",
    "# 1.96 because that is the 95% confidence interval\n",
    "plt.errorbar(x,pixel,yerr=pixel_sd*1.96)\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would generally expect LAI to be quite smoothly varying over time. Visualising the data with 95% confidence intervals is quite useful as we can now 'imagine' some smooth line that would generally go within these bounds.\n",
    "\n",
    "Some of the uncertainty estimates are really rather small though, which are probably not reliable.\n",
    "\n",
    "Let's inflate them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data = lai['Lai_1km'].copy()\n",
    "sd   = lai['LaiStdDev_1km'].copy()\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel    = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "# threshold\n",
    "thresh = 0.25\n",
    "pixel_sd[pixel_sd<thresh] = thresh\n",
    "\n",
    "x = np.arange(len(pixel))*8\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(x,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(x,pixel,'k--')\n",
    "# plot error bars:\n",
    "# 1.96 because that is the 95% confidence interval\n",
    "plt.errorbar(x,pixel,yerr=pixel_sd*1.96)\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps a bit more realistic ...\n",
    "\n",
    "The data now have some missing values (data gaps) and, as we have noted, are a little noisy.\n",
    "\n",
    "A Python module we can use for many scientific functions is [`scipy`](http://docs.scipy.org/doc/scipy), in particular here, the [`scipy` interpolation functions](http://docs.scipy.org/doc/scipy/reference/interpolate.html).\n",
    "\n",
    "We need to make a careful choice of the interpolation functions.\n",
    "\n",
    "We might, in many circumstances simply want something that interpolates between data points, i.e. that goes through the data points that we have.\n",
    "\n",
    "Many interpolators will not provide extrapolation, so in the example above we could not get an estimate of LAI prior to the first sample and after the last.\n",
    "\n",
    "The best way to deal with that would be to have multiple years of data.\n",
    "\n",
    "Instead here, we will repeat the dataset three times to mimic this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "\n",
    "# original x,y\n",
    "y_ = pixel\n",
    "x_ = (np.arange(len(y_))*8.+1)[~pixel.mask]\n",
    "y_ = y_[~pixel.mask]\n",
    "\n",
    "# extend: using np.tile() to repeat data\n",
    "y_extend = np.tile(y_,3)\n",
    "# extend: using vstack to stack 3 different arrays\n",
    "x_extend = np.hstack((x_-46*8,x_,x_+46*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the extended dataset\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(x_extend,y_extend,'b')\n",
    "plt.plot(x_,y_,'k+')\n",
    "plt.plot([0.,0.],[0.,2.5],'r')\n",
    "plt.plot([365.,365.],[0.,2.5],'r')\n",
    "plt.xlim(-356,2*365)\n",
    "plt.xlabel('day of year')\n",
    "plt.ylabel('LAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define xnew at 1 day interval\n",
    "xnew = np.arange(1.,366.)\n",
    "\n",
    "# linear interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "ynew = f(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cubic interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='cubic')\n",
    "ynew = f(xnew)\n",
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nearest neighbour interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='nearest')\n",
    "ynew = f(xnew)\n",
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the problem you are trying to solve, different interpolation schemes will be appropriate. For categorical data (e.g. 'snow', coded as 1 and 'no snow' coded as 1), for instance, a nearest neighbour interpolation might be a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the schemes above is that they go exactly through the data points, but a more realistic description of the data might be one that incorporated the uncertainty information we have. Visually, this is quite easy to imagine, but how can we implement such ideas?\n",
    "\n",
    "One way of thinking about this is to think about other sources of information that we might bring to bear on the problem. One such would be that we expect the function to be 'quite smooth'. This allows us to consider applying smoothness as an additional constraint to the solution.\n",
    "\n",
    "Many such problems can be phrased as convolution operations.\n",
    "\n",
    "Convolution is a form of digital filtering that combines two sequences of numbers $y$ and $w$ to give a third, the result $z$ that is a filtered version of $y$, where for each element $j$ of $y$:\n",
    "\n",
    "$$\n",
    "  z_j = \\sum_{i=-n}^{i=n}{w_i y_{j+i}}\n",
    "$$\n",
    "\n",
    "where $n$ is the half width of the filter $w$. For a smoothing filter, the elements of this will sum to 1 (so that the magnitude of $y$ is not changed).\n",
    "\n",
    "To illustrate this in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple box smoothing filter\n",
    "# filter width 11\n",
    "w = np.ones(11)\n",
    "# normalise\n",
    "w = w/w.sum()\n",
    "# half width\n",
    "n = len(w)/2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "# where we will put the result\n",
    "z = np.zeros_like(y)\n",
    "\n",
    "# This is a straight implementation of the\n",
    "# equation above\n",
    "for j in xrange(n,len(y)-n):\n",
    "    for i in xrange(-n,n+1):\n",
    "        z[j] += w[n+i] * y[j+i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d'%len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suggested, the result of convolving $y$ with the filter $w$ (of width 31 here) is $z$, a smoothed version of $y$.\n",
    "\n",
    "You might notice that the filter is only applied once we are `n` samples into the signal, so we get 'edge effects'. There are various ways of dealing with edge effects, such as repeating the signal (as we did above, for much the same reason), reflecting the signal, or assuming the signal to be some constant value (e.g. 0) outside of its defined domain.\n",
    "\n",
    "If we make the filter wider (width 31 now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple box smoothing filter\n",
    "# filter width 31\n",
    "w = np.ones(31)\n",
    "# normalise\n",
    "w = w/w.sum()\n",
    "# half width\n",
    "n = len(w)/2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "# where we will put the result\n",
    "z = np.zeros_like(y)\n",
    "\n",
    "# This is a straight implementation of the\n",
    "# equation above\n",
    "for j in xrange(n,len(y)-n):\n",
    "    for i in xrange(-n,n+1):\n",
    "        z[j] += w[n+i] * y[j+i]\n",
    "        \n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d'%len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the signal is 'more' smoothed. \n",
    "\n",
    "There are *many* filters implemented in [`scipy.signal`](http://docs.scipy.org/doc/scipy/reference/signal.html) that you should look over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very commonly used smoothing filter is the [Savitsky-Golay](http://en.wikipedia.org/wiki/Savitzkyâ€“Golay_filter_for_smoothing_and_differentiation) filter for which you define the window size and filter order.\n",
    "\n",
    "As with most filters, the filter width controls the degree of smoothing (see examples above). The filter order (related to polynomial order) in essence controls the shape of the filter and defines the 'peakiness' of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "# see http://wiki.scipy.org/Cookbook/SavitzkyGolay\n",
    "from savitzky_golay import *\n",
    "\n",
    "window_size = 31\n",
    "order = 1\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "z = savitzky_golay(y,window_size,order)\n",
    "\n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d order %.2f'%(window_size,order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "# see http://wiki.scipy.org/Cookbook/SavitzkyGolay\n",
    "from savitzky_golay import *\n",
    "\n",
    "window_size = 61\n",
    "order = 2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "z = savitzky_golay(y,window_size,order)\n",
    "\n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d order %.2f'%(window_size,order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the samples $y$ have uncertainty (standard deviation $\\sigma_j$ for sample $j$) associated with them, we can incorporate this into smoothing, although many of the methods in `scipy` and `numpy` do not directly allow for this.\n",
    "\n",
    "Instead, we call an optimal interpolation scheme (a regulariser) here that achieves this. This also has the advantage of giving an estimate of uncertainty for the smoothed samples.\n",
    "\n",
    "In this case, the parameters are: `order` (as above, but only integer in this implementation) and `wsd` which is an estimate of the variation (standard deviation) in the signal that control smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tile = 'h17v03'\n",
    "year = '2005'\n",
    "\n",
    "# specify the file with the urls in\n",
    "ifile= 'data/modis_lai_%s_%s.txt'%(tile,year)\n",
    "\n",
    "fp = open(ifile)\n",
    "filelist = [url.split('/')[-1].strip() for url in fp.readlines()]\n",
    "fp.close()\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "\n",
    "from get_lai import *\n",
    "\n",
    "try:\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "except:\n",
    "    lai = read_lai(filelist,country='IRELAND')\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "    \n",
    "thresh = 0.25\n",
    "sd[sd<thresh] = thresh\n",
    "\n",
    "r = 472\n",
    "c = 84\n",
    "from smoothn import *\n",
    "\n",
    "# this is about the right amount of smoothing here\n",
    "gamma = 5.\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "z = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=2.0)[0]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "# lower and upper bounds of 95% CI\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.ylim(0.,2.5)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it on a new pixel\n",
    "\n",
    "r = 472\n",
    "c = 86\n",
    "\n",
    "gamma = 5\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "z = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=2.0)[0]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.legend(loc='best')\n",
    "z.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and test it on a new pixel\n",
    "\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "#r = 9\n",
    "#c = 277\n",
    "gamma = 5.\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "# solve for gamma - degree of smoothness \n",
    "zz = smoothn(pixel,sd=pixel_sd,smoothOrder=2.0)\n",
    "z = zz[0]\n",
    "print zz[1],zz[2]\n",
    "\n",
    "gamma = zz[1]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply this approach to our 3D dataset, we could simply loop over all pixels.\n",
    "\n",
    "Note that *any* per-pixel processing will be slow ... but this is quite a fast smoothing method, so is feasible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have put in an axis control to smoothn\n",
    "# here so it will only smooth over doy\n",
    "# This will take a few minutes to process\n",
    "# we switch on verbose mode to get some feedback\n",
    "# on progress\n",
    "\n",
    "# make a mask of pixels where there is at least 1 sample\n",
    "# over the time period\n",
    "mask = (data.mask.sum(axis=0) == 0)\n",
    "mask = np.array([mask]*data.shape[0])\n",
    "\n",
    "z = smoothn(data,s=5.0,sd=sd,smoothOrder=2.0,axis=0,TolZ=0.05,verbose=True)[0]\n",
    "z = ma.array(z,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(z[20],interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarly, take frame 20\n",
    "# and smooth that\n",
    "\n",
    "ZZ = smoothn(z[20],smoothOrder=2.)\n",
    "# self-calibrated smoothness term\n",
    "s = ZZ[1]\n",
    "print 's =',s\n",
    "Z = ZZ[0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(Z,interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarly, take frame 20\n",
    "# and smooth that\n",
    "\n",
    "ZZ = smoothn(z,s=s,smoothOrder=2.,axis=(1,2),verbose=True)\n",
    "\n",
    "Z = ZZ[0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(Z[30],interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(46)*8+1.\n",
    "try:\n",
    "    plt.plot(x,np.mean(Z,axis=(1,2)))\n",
    "    plt.plot(x,np.min(Z,axis=(1,2)),'r--')\n",
    "    plt.plot(x,np.max(Z,axis=(1,2)),'r--')\n",
    "except:\n",
    "    plt.plot(x,np.mean(Z,axis=2).mean(axis=1))\n",
    "    plt.plot(x,np.min(Z,axis=2).min(axis=1),'r--')\n",
    "    plt.plot(x,np.max(Z,axis=2).max(axis=1),'r--')\n",
    "    \n",
    "plt.title('LAI variation of Eire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or doing this pixel by pixel ...\n",
    "# which is slower than using axis\n",
    "\n",
    "order = 2\n",
    "\n",
    "# pixels that have some data\n",
    "mask = (~data.mask).sum(axis=0)\n",
    "\n",
    "odata = np.zeros((46,) + mask.shape)\n",
    "\n",
    "rows,cols = np.where(mask>0)\n",
    "\n",
    "len_x = len(rows)\n",
    "order = 2\n",
    "gamma = 5.\n",
    "\n",
    "for i in xrange(len_x):\n",
    "    r,c = rows[i],cols[i]\n",
    "    # progress bar\n",
    "    if i%(len_x/20) == 0:\n",
    "        print '... %4.2f percent'%(i*100./float(len_x))\n",
    "    pixel    = data[:,r,c]\n",
    "    pixel_sd = sd[:,r,c]\n",
    "\n",
    "    zz = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=order,TolZ=0.05)\n",
    "    odata[:,rows[i],cols[i]] = zz[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "root = 'images/lai_eire_colourZ'\n",
    "\n",
    "for i,f in enumerate(lai['filenames']):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    # get some info from filename\n",
    "    file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "    print file_id\n",
    "    plt.imshow(Z[i],interpolation='none',vmax=6.,vmin=0.0)\n",
    "    # plot a jpg\n",
    "    plt.title(file_id)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie2.gif'.format(root)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_eire_colourZ_movie2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Function fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of applying some arbitrary smoothing function to data, we want to extract particular infromation from the time series.\n",
    "\n",
    "One way to approach this is to fit some function to the time series at each location.\n",
    "\n",
    "Let us suppose that we wish to characterise the phenology of vegetation in Ireland.\n",
    "\n",
    "![](http://www2.geog.ucl.ac.uk/~plewis/geogg124/_images/zhang1.png)\n",
    "\n",
    "One way we could do this would be to look in the lai data for the most rapid changes.\n",
    "\n",
    "Another would be to explicitly fit some mathematical function to the LAI data that would would expect to descrive typical LAI trajectories.\n",
    "\n",
    "One example of such a function is the double logistic. A logistic function is:\n",
    "\n",
    "$$\n",
    " \\hat{y} = p_0 - p_1 \\left( \\frac{1}{1 + e^{p_2 (t - p_3)}} + \\frac{1}{1 + e^{p_4 (t - p_5)}} -1\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give a function for a double logistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dbl_logistic_model ( p, t ):\n",
    "        \"\"\"A double logistic model, as in Sobrino and Juliean, \n",
    "        or Zhang et al\"\"\"\n",
    "        return p[0] - p[1]* ( 1./(1+np.exp(p[2]*(t-p[3]))) + \\\n",
    "                              1./(1+np.exp(-p[4]*(t-p[5])))  - 1 )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tile = 'h17v03'\n",
    "year = '2005'\n",
    "\n",
    "# specify the file with the urls in\n",
    "ifile= 'data/modis_lai_%s_%s.txt'%(tile,year)\n",
    "\n",
    "fp = open(ifile)\n",
    "filelist = [url.split('/')[-1].strip() for url in fp.readlines()]\n",
    "fp.close()\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "\n",
    "from get_lai import *\n",
    "\n",
    "try:\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "except:\n",
    "    lai = read_lai(filelist,country='IRELAND')\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "    \n",
    "thresh = 0.25\n",
    "sd[sd<thresh] = thresh\n",
    "\n",
    "# test pixel\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "\n",
    "y = data[:,r,c]\n",
    "mask = ~y.mask\n",
    "y = np.array(y[mask])\n",
    "x = (np.arange(46)*8+1.)[mask]\n",
    "unc = np.array(sd[:,r,c][mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define x (time)\n",
    "x_full = np.arange(1,366) \n",
    "\n",
    "# some default values for the parameters\n",
    "p = np.zeros(6)\n",
    "\n",
    "# some stats on y\n",
    "ysd = np.std(y)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "# some rough guesses at the parameters\n",
    "\n",
    "p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "p[1] = 2*1.151*ysd          # range\n",
    "p[2] = 0.19                 # related to up slope\n",
    "p[3] = 120                  # midpoint of up slope\n",
    "p[4] = 0.13                 # related to down slope\n",
    "p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "y_hat = dbl_logistic_model(p,x_full)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_full,y_hat)\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually 'tweak' the parameters until we got a better 'fit' to the observations.\n",
    "\n",
    "First though, let's define a measure of 'fit':\n",
    "\n",
    "$$\n",
    "Z_i = \\frac{\\hat{y}_i - y_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^2 = \\sum_i{Z_i^2} =  \\sum_i{\\left( \\frac{\\hat{y}_i - y_i}{\\sigma_i} \\right)^2}\n",
    "$$\n",
    "\n",
    "and implement this as a mismatch function where we have data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mismatch_function(p, x, y, unc):\n",
    "    y_hat = dbl_logistic_model(p, x)\n",
    "    diff = (y_hat - y)/unc\n",
    "    return diff\n",
    "\n",
    "\n",
    "Z = mismatch_function(p,x,y,unc)\n",
    "\n",
    "plt.plot([1,365.],[0,0.],'k-')\n",
    "plt.xlim(0,365)\n",
    "plt.plot(x,Z,'*')\n",
    "\n",
    "\n",
    "print 'Z^2 =',(Z**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change p a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "p[1] = 2*1.151*ysd          # range\n",
    "p[2] = 0.19                 # related to up slope\n",
    "p[3] = 140                  # midpoint of up slope\n",
    "p[4] = 0.13                 # related to down slope\n",
    "p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "Z = mismatch_function(p,x,y,unc)\n",
    "\n",
    "plt.plot([1,365.],[0,0.],'k-')\n",
    "plt.xlim(0,365)\n",
    "plt.plot(x,Z,'*')\n",
    "\n",
    "\n",
    "print 'Z^2 =',(Z**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made the mismatch go down a little ...\n",
    "\n",
    "Clearly it would be tedious (and impractical) to do a lot of such tweaking, so we can use methods that seek the minimum of some function.\n",
    "\n",
    "One such method is implemented in `scipy.optimize.leastsq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# initial estimate is in p\n",
    "print 'initial parameters:',p[0],p[1],p[2],p[3],p[4],p[5]\n",
    "\n",
    "# set some bounds for the parameters\n",
    "bound = np.array([(0.,10.),(0.,10.),(0.01,1.),\\\n",
    "                  (50.,300.),(0.01,1.),(50.,300.)])\n",
    "\n",
    "\n",
    "# test pixel\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "\n",
    "y = data[:,r,c]\n",
    "mask = ~y.mask\n",
    "y = np.array(y[mask])\n",
    "x = (np.arange(46)*8+1.)[mask]\n",
    "unc = np.array(sd[:,r,c][mask])\n",
    "\n",
    "# define function to give Z^2\n",
    "\n",
    "def sse(p,x,y,unc):\n",
    "    '''Sum of squared error'''\n",
    "    # penalise p[3] > p[5]\n",
    "    err = np.max([0.,(p[3] - p[5])])*1e4\n",
    "    return (mismatch_function(p,x,y,unc)**2).sum()+err\n",
    "\n",
    "# we pass the function:\n",
    "#\n",
    "# sse               : the name of the function we wrote to give \n",
    "#                     sum of squares of Z_i\n",
    "# p                 : an initial estimate of the parameters\n",
    "# args=(x,y,unc)    : the other information (other than p) that\n",
    "#                     mismatch_function needs\n",
    "# approx_grad       : if we dont have a function for the gradien\n",
    "#                     we have to get the solver to approximate it\n",
    "#                     which takes time ... see if you can work out\n",
    "#                     d_sse / dp and use that to speed this up!\n",
    "\n",
    "psolve = optimize.fmin_l_bfgs_b(sse,p,approx_grad=True,iprint=-1,\\\n",
    "                                args=(x,y,unc),bounds=bound)\n",
    "\n",
    "print psolve[1]\n",
    "pp = psolve[0]\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)\n",
    "y_hat = dbl_logistic_model(pp,x_full)\n",
    "plt.plot(x_full,y_hat)\n",
    "\n",
    "print 'solved parameters: ',pp[0],pp[1],pp[2],pp[3],pp[4],pp[5]\n",
    "\n",
    "# if we define the phenology as the parameter p[3]\n",
    "# and the 'length' of the growing season:\n",
    "print 'phenology',pp[3],pp[5]-pp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and run over each pixel ... this will take some time\n",
    "\n",
    "# pixels that have some data\n",
    "mask = (~data.mask).sum(axis=0)\n",
    "\n",
    "pdata = np.zeros((7,) + mask.shape)\n",
    "\n",
    "rows,cols = np.where(mask>0)\n",
    "len_x = len(rows)\n",
    "\n",
    "# lets just do some random ones to start with\n",
    "#rows = rows[::10]\n",
    "#cols = cols[::10]\n",
    "\n",
    "len_x = len(rows)\n",
    "\n",
    "\n",
    "for i in xrange(len_x):\n",
    "    r,c = rows[i],cols[i]\n",
    "    # progress bar\n",
    "    if i%(len_x/40) == 0:\n",
    "        print '... %4.2f percent'%(i*100./float(len_x))\n",
    "    \n",
    "    y = data[:,r,c]\n",
    "    mask = ~y.mask\n",
    "    y = np.array(y[mask])\n",
    "    x = (np.arange(46)*8+1.)[mask]\n",
    "    unc = np.array(sd[:,r,c][mask])\n",
    "    \n",
    "    # need to get an initial estimate of the parameters\n",
    "    \n",
    "    # some stats on y\n",
    "    ysd = np.std(y)\n",
    "    ymean = np.mean(y)\n",
    "\n",
    "    p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "    p[1] = 2*1.151*ysd          # range\n",
    "    p[2] = 0.19                 # related to up slope\n",
    "    p[3] = 140                  # midpoint of up slope\n",
    "    p[4] = 0.13                 # related to down slope\n",
    "    p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "    \n",
    "    # set factr to quite large number (relative error in solution)\n",
    "    # as it'll take too long otherwise\n",
    "    psolve = optimize.fmin_l_bfgs_b(sse,p,approx_grad=True,iprint=-1,\\\n",
    "                                args=(x,y,unc),bounds=bound,factr=1e12)\n",
    "\n",
    "    pdata[:-1,rows[i],cols[i]] = psolve[0]\n",
    "    pdata[-1,rows[i],cols[i]] = psolve[1] # sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[3],interpolation='none',vmin=137,vmax=141)\n",
    "plt.title('green up doy')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[5]-pdata[3],interpolation='none',vmin=74,vmax=84)\n",
    "plt.title('season length')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[0],interpolation='none',vmin=0.,vmax=6.)\n",
    "plt.title('min LAI')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[1]+pdata[0],interpolation='none',vmin=0.,vmax=6.)\n",
    "plt.title('max LAI')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.sqrt(pdata[-1]),interpolation='none',vmax=np.sqrt(500))\n",
    "plt.title('RSSE')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check a few pixels\n",
    "\n",
    "c = 200\n",
    "\n",
    "for r in xrange(200,400,25):\n",
    "    y = data[:,r,c]\n",
    "    mask = ~y.mask\n",
    "    y = np.array(y[mask])\n",
    "    x = (np.arange(46)*8+1.)[mask]\n",
    "    unc = np.array(sd[:,r,c][mask])\n",
    "    \n",
    "    x_full = np.arange(1,366) \n",
    "    \n",
    "    # some default values for the parameters\n",
    "    pp = pdata[:-1,r,c]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.title('r %d c %d'%(r,c))\n",
    "    plt.plot(x,y,'*')\n",
    "    plt.errorbar(x,y,unc*1.96)\n",
    "    y_hat = dbl_logistic_model(pp,x_full)\n",
    "    plt.plot(x_full,y_hat)\n",
    "    \n",
    "    print 'solved parameters: ',pp[0],pp[1],pp[2],pp[3],pp[4],pp[5]\n",
    "    \n",
    "    # if we define the phenology as the parameter p[3]\n",
    "    # and the 'length' of the growing season:\n",
    "    print 'phenology',pp[3],pp[5]-pp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
